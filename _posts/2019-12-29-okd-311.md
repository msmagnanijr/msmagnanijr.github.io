---
layout: post
title: Openshift OKD 3.11 - Setting up a High Availability Environment-#WIP
tags: [okd, openshift, devops, redhat, centos, ha]
comments: true
---

## Introduction

OpenShift is Red Hat's Cloud Computing Platform as a Service PaaS that offers several alternatives for developers to build, test and run applications quickly, simply and scalably.

OpenShift “takes care” of all the complexity related to infrastructure, middleware, and management, keeping the developer focused on application design and coding.

Currently the latest version is 4.x which really brought numerous facilities like Installer Provisioned Infrastructure (IPI), which makes it much easier for the most inexperienced. In this article we will focus on version 3.x because it is an already established version and has been widely used in the last 4 years.

Explaining some terms:

* OpenShift Enterprise - Available only to customers who have purchased a Red Hat subscription. This version can be installed and configured on Amazon, Azure, or any company's internal infrastructure.

* OpenShift OKD - Available in the community where any / sysadmin developer can install and configure whether on Amazon, Azure or internal infrastructure, is not officially supported and is not offered any SLA.

* OpenShift Online - Online version where the developer pays for “usage” (CPU, Memory, etc.). Interaction is performed through the command line.

* OpenShift Dedicated - Version used by large companies that do not want to have the work of installing, configuring, upgrading, etc., leaving this task to the Red Hat engineering team.

* Openshift Minishift - Version intended for local execution only. It is used by developers to observe application behavior prior to shipping to the Openshift cluster.

Using OpenShift you can quickly see the following advantages:

* Self-service platform - Developers can create their own applications in different languages (Polyglot, multilanguage support) where resource utilization limits and quotas can be set for each project.

* Scalable - You can scale to dozens of PODs in just a few minutes and this automatically.

* Open Source / Choice of cloud - In addition to no lock-in vendor, you can deploy across multiple platforms with Amazon, Microsoft Azure, Google and even On-Premise.

## OpenShift Platform Key Concepts

* Containers - An instance running as a JBoss, Wordpress, Mysql, etc. application.

* Pods - Set of one or more containers and management layers.

* Nodes - RHEL/CentOS or CoreOS instance where the application is currently running.

* Masters - RHEL/CentOS or CoreOS instance which aims to manage everything that happens in the cluster.

Briefly we can visualize the following flow:

- Applications are packaged in docker images;
- Applications are executed in containers and grouped in Pods;
- Pod / containers run on Nodes;
- Nodes are managed by the Master;

## Installing Openshift OKD 3.11

As I said earlier, OpenShift can be installed on many platforms, so for this article's tests we will use oVirt. [oVirt](https://www.ovirt.org) is a free open-source virtualization platform.

As I want to show how to create a productive environment, I will configure the "HA" of the management layer (Master) and also the "HA" of the application layer (Router).

Prerequisites:

* The servers below must have CentOS 7.7 installed 

* DNS needs to be working perfectly. The servers need to talk to each other using "names".

* An extra disk attached to the server.

 I will use the following servers:

 * master-01.mmagnani.lab - 10.0.0.80
 * master-02.mmagnani.lab - 10.0.0.81
 * master-03.mmagnani.lab - 10.0.0.82
 * node-01.mmagnani.lab   - 10.0.0.83
 * node-02.mmagnani.lab   - 10.0.0.84
 * infra-01.mmagnani.lab  - 10.0.0.85
 * infra-02.mmagnani.lab  - 10.0.0.86
 * balancer.mmagnani.lab  - 10.0.0.87
 * bastion.mmagnani.lab   - 10.0.0.116

We will execute all commands from the "bastion.mmagnani.lab" server, as the name already says it will be our central execution point. The first step of course is to install Ansible and create our inventory so that commands can be executed on all nodes:

{% highlight bash %}
[root@bastion ~]# yum update -y
[root@bastion ~]# yum install https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.9-1.el7.ans.noarch.rpm -y
[root@bastion ~]# mkdir -p /root/workspace/okd
[root@bastion ~]# cd /root/workspace/okd
[root@bastion ~]# vi inventory

[all]
master-01.mmagnani.lab
master-02.mmagnani.lab
master-03.mmagnani.lab
node-01.mmagnani.lab
node-02.mmagnani.lab
infra-01.mmagnani.lab
infra-02.mmagnani.lab
balancer.mmagnani.lab

[root@bastion ~]# ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
[root@bastion ~]# ssh-copy-id master-01.mmagnani.lab
[root@bastion ~]# ssh-copy-id master-02.mmagnani.lab
[root@bastion ~]# ssh-copy-id master-03.mmagnani.lab
[root@bastion ~]# ssh-copy-id node-01.mmagnani.lab 
[root@bastion ~]# ssh-copy-id node-02.mmagnani.lab 
[root@bastion ~]# ssh-copy-id infra-01.mmagnani.lab 
[root@bastion ~]# ssh-copy-id infra-02.mmagnani.lab 
[root@bastion ~]# ssh-copy-id balancer.mmagnani.lab 
{% endhighlight %}

We now have access to execute commands on all nodes using Ansible. You can test by pinging:

{% highlight bash %}
[root@bastion ~]# ansible all -i inventory -m ping
master-02.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
node-01.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
infra-01.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
infra-02.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
balancer.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
node-02.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
master-03.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
master-01.mmagnani.lab | SUCCESS => {
    "changed": false, 
    "ping": "pong"
}
{% endhighlight %}

Update all nodes, run:

{% highlight bash %}
[root@bastion ~]# ansible all -i inventory -m shell -a "yum update -y"
{% endhighlight %}

Selinux must be enforcing on all nodes, run:

{% highlight bash %}
[root@bastion ~]# ansible all -i inventory -m shell -a "getenforce"
{% endhighlight %}

Firewalld in this case must be disabled on all nodes, run: 

{% highlight bash %}
[root@bastion ~]# ansible all -i inventory -m shell -a "systemctl stop firewalld"
[root@bastion ~]# ansible all -i inventory -m shell -a "systemctl disable firewalld"
{% endhighlight %}

Install EPEL repo on all nodes, run:

{% highlight bash %}
[root@bastion ~]# ansible all -i inventory -m shell -a "yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm -y"
[root@bastion ~]# ansible all -i inventory -m shell -a "sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo"
{% endhighlight %}

Install the following packages on all nodes:

{% highlight bash %}
[root@bastion ~]# ansible all -i inventory -m shell -a "yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct -y"
[root@bastion ~]# ansible all -i inventory -m shell -a "yum -y --enablerepo=epel install pyOpenSSL -y"
{% endhighlight %}

Best,#WIP