---
layout: post
title: Java High Availability with WildFly on Kubernetes
tags: [kubernetes, k8s, devops, centos, wildfly]
comments: true
---

## Legacy Java Applications

With the advancement of dockerization of applications, Kubernetes has become a standard in the market but we must remember that there are still thousands of legacy applications that depend on certain features provided by the application servers. So if you need to use session replication you will surely need Wildfly instances to be clustered. To solve this smoothly you can use thek "Kubernetes discovery protocol for JGroups" aka "KUBE-PING". KUBE_PING is a discovery protocol for JGroups cluster nodes managed by Kubernetes: [jgroups-kubernetes](https://github.com/jgroups-extras/jgroups-kubernetes)

## Walkthrough

I assume you already have a Kubernetes cluster so let's just focus on WildFly settings.

The first step is to create a repository that will contain all our files:

{% highlight bash %}
[root@workstation ~]# mkdir wildfly18-kubeping
[root@workstation ~]# mkdir -p wildfly18-kubeping/application
[root@workstation ~]# mkdir -p wildfly18-kubeping/configuration
[root@workstation ~]# mkdir -p wildfly18-kubeping/kubernetes
{% endhighlight %}

Then create Dockerfile with the necessary steps for image customization:

{% highlight bash %}
[root@workstation ~]# vi wildfly18-kubeping/Dockerfile
FROM jboss/wildfly:18.0.1.Final

LABEL MAINTAINER Mauricio Magnani <msmagnanijr@gmail.com>

RUN /opt/jboss/wildfly/bin/add-user.sh admin redhat --silent

ADD configuration/config-server.cli /opt/jboss/

RUN /opt/jboss/wildfly/bin/jboss-cli.sh --file=config-server.cli

RUN rm -Rf /opt/jboss/wildfly/standalone/configuration/standalone_xml_history/*

ADD application/cluster.war /opt/jboss/wildfly/standalone/deployments/

EXPOSE 8080 9990 7600 8888
{% endhighlight %}

In the same directory, create the file "config-server.cli" which will contain the steps to configure kubeping correctly:

{% highlight bash %}
[root@workstation ~]# vi wildfly18-kubeping/configuration/config-server.cli
###start the server in admin-only mode, using/modifying standalone-full-ha.xml
embed-server --server-config=standalone-full-ha.xml --std-out=echo

###apply all configuration to the server
batch
#/subsystem=logging/logger=org.openshift.ping:add()
#/subsystem=logging/logger=org.openshift.ping:write-attribute(name=level, value=DEBUG)
#/subsystem=logging/logger=org.openshift.ping:add-handler(name=CONSOLE)
/subsystem=messaging-activemq/server=default/cluster-connection=my-cluster:write-attribute(name=reconnect-attempts,value=10)  
/interface=kubernetes:add(nic=eth0)  
/socket-binding-group=standard-sockets/socket-binding=jgroups-tcp/:write-attribute(name=interface,value=kubernetes)  
/socket-binding-group=standard-sockets/socket-binding=jgroups-tcp-fd/:write-attribute(name=interface,value=kubernetes)  
/subsystem=jgroups/channel=ee:write-attribute(name=stack,value=tcp)
/subsystem=jgroups/channel=ee:write-attribute(name=cluster,value=kubernetes)
/subsystem=jgroups/stack=tcp/protocol=MPING:remove()
/subsystem=jgroups/stack=tcp/protocol=kubernetes.KUBE_PING:add(add-index=0,properties={namespace=${env.KUBERNETES_NAMESPACE},labels=${env.KUBERNETES_LABELS},port_range=0,masterHost=kubernetes.default.svc,masterPort=443})
run-batch

###stop embedded server
stop-embedded-server
{% endhighlight %}

Now put the "package of your application" to the directory "application"

{% highlight bash %}
[root@workstation ~]# cp cluster.war wildfly18-kubeping/application
{% endhighlight %}

The next step is to create all Kubernetes objects so we can do the tests later(ps: I'm using the namespace "labs"):

{% highlight yaml %}
[root@workstation ~]# vi wildfly18-kubeping/kubernetes/01-wildfly-sa-role.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: jgroups-kubeping-service-account
  namespace: labs
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: jgroups-kubeping-pod-reader
  namespace: labs
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: jgroups-kubeping-api-access
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: jgroups-kubeping-pod-reader
subjects:
- kind: ServiceAccount
  name: jgroups-kubeping-service-account
  namespace: labs

[root@workstation ~]# vi wildfly18-kubeping/kubernetes/02-wildfly-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: wildfly
  namespace: labs
  labels:
    app: wildfly
    tier: devops
spec:
  selector:
    matchLabels:
      app: wildfly
      tier: devops
  replicas: 2
  template: 
    metadata:
      labels:
        app: wildfly
        tier: devops
    spec:
      serviceAccountName: jgroups-kubeping-service-account
      containers:
        - name: kube-ping
          image: mmagnani/wildfly18-kubeping:latest
          command: ["/opt/jboss/wildfly/bin/standalone.sh"]
          args: ["--server-config", "standalone-full-ha.xml", "-b", $(POD_IP), "-bmanagement", $(POD_IP) ,"-bprivate", $(POD_IP) ]
          resources:
            requests:
              memory: 256Mi
            limits:
              memory: 512Mi
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
            - containerPort: 9990
            - containerPort: 7600
            - containerPort: 8888
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: KUBERNETES_LABELS 
              value: app=wildfly

[root@workstation ~]# vi wildfly18-kubeping/kubernetes/03-wildfly-service.yaml

apiVersion: v1
kind: Service
metadata:
    name: wildfly
    namespace: labs
    labels:
      app: wildfly
      tier: devops
spec:
  type: ClusterIP
  ports:
    - targetPort: 8080
      port: 8080
  selector:
    app: wildfly
    tier: devops

[root@workstation ~]# vi wildfly18-kubeping/kubernetes/04-wildfly-ingress.yaml

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
    name: wildfly-ingress
    namespace: labs
spec:
  rules:
  - host: wildfly.mmagnani.lab
    http:
      paths:
      - path: /
        backend:
          serviceName: wildfly
          servicePort: 8080

{% endhighlight %}

Now just run the image build and push this to the docker hub or any other "registry":

{% highlight bash %}
[root@workstation ~]# docker build -t mmagnani/wildfly18-kubeping:latest . 
[root@workstation ~]# docker push mmagnani/wildfly18-kubeping
{% endhighlight %}

In the context of your Kubernetes cluster just create those objects:

{% highlight bash %}
[root@k8s-master kubernetes]# kubelet --version
Kubernetes v1.16.1
[root@k8s-master kubernetes]# pwd
/root/workspace/wildfly18-kubeping/kubernetes
[root@k8s-master kubernetes]# kubectl create -f .
[root@k8s-master kubernetes]# kubectl get pods -n labs
NAME                      READY   STATUS    RESTARTS   AGE
wildfly-fd6dfb6cd-nm4pf   1/1     Running   0          28m
wildfly-fd6dfb6cd-vvzwh   1/1     Running   0          28m
{% endhighlight %}

Check the logs, you should find two members as we only define 2 replicas:

{% highlight bash %}
[root@k8s-master kubernetes]# kubectl logs -f wildfly-fd6dfb6cd-nm4pf -n labs

13:15:39,705 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: WildFly Full 18.0.1.Final (WildFly Core 10.0.3.Final) started in 10972ms - Started 570 of 776 services (477 services are lazy, passive or on-demand)
13:16:17,782 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN000094: Received new cluster view for channel kubernetes: [wildfly-fd6dfb6cd-6lvr4|1] (2) [wildfly-fd6dfb6cd-6lvr4, wildfly-fd6dfb6cd-zw6j2]
13:16:17,786 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN100000: Node wildfly-fd6dfb6cd-zw6j2 joined the cluster
13:16:17,788 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN000094: Received new cluster view for channel kubernetes: [wildfly-fd6dfb6cd-6lvr4|1] (2) [wildfly-fd6dfb6cd-6lvr4, wildfly-fd6dfb6cd-zw6j2]
13:16:17,788 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN100000: Node wildfly-fd6dfb6cd-zw6j2 joined the cluster
13:16:17,789 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN000094: Received new cluster view for channel kubernetes: [wildfly-fd6dfb6cd-6lvr4|1] (2) [wildfly-fd6dfb6cd-6lvr4, wildfly-fd6dfb6cd-zw6j2]
13:16:17,790 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN100000: Node wildfly-fd6dfb6cd-zw6j2 joined the cluster
13:16:17,790 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN000094: Received new cluster view for channel kubernetes: [wildfly-fd6dfb6cd-6lvr4|1] (2) [wildfly-fd6dfb6cd-6lvr4, wildfly-fd6dfb6cd-zw6j2]
13:16:17,790 INFO  [org.infinispan.CLUSTER] (thread-5,null,wildfly-fd6dfb6cd-6lvr4) ISPN100000: Node wildfly-fd6dfb6cd-zw6j2 joined the cluster
13:16:18,758 INFO  [org.infinispan.CLUSTER] (remote-thread--p3-t1) [Context=default-server] ISPN100002: Starting rebalance with members [wildfly-fd6dfb6cd-6lvr4, wildfly-fd6dfb6cd-zw6j2], phase READ_OLD_WRITE_ALL, topology id 2
{% endhighlight %}

As I am using "Traefik" my "ingress" is available:

![](/images/202001-kubeha-01.png)

In the application it is also possible to add object to the session:

![](/images/202001-kubeha-02.png)

All files are available in the repository: https://github.com/msmagnanijr/wildfly18-kubeping.git

## Issue on Kubernetes 1.17

Unfortunately these settings will not work in Kubernetes 1.17 as there is a mismatch between TLS 1.3 Kubernetes/Golang and TLS 1.2 from Java/WildFly. In the logs you will see something like this:

{% highlight bash %}
12:39:26,943 WARN  [org.jgroups.protocols.kubernetes.KUBE_PING] (thread-10,kubernetes,wildfly-deployment-f8d595bb5-s6ljn) failed getting JSON response from Kubernetes Client[masterUrl=https://kubernetes.default.svc:443/api/v1, headers={Authorization=#MASKED:999#}, connectTimeout=5000, readTimeout=30000, operationAttempts=3, operationSleep=1000, streamProvider=org.jgroups.protocols.kubernetes.stream.InsecureStreamProvider@39127235] for cluster [kubernetes], namespace [labs], labels [app=wildfly]; encountered [java.lang.Exception: 3 attempt(s) with a 1000ms sleep to execute [OpenStream] failed. Last failure was [javax.net.ssl.SSLHandshakeException: extension (5) should not be presented in certificate_request]]
{% endhighlight %}

For more information please check:

* https://bugs.openjdk.java.net/browse/JDK-8234949
* https://github.com/golang/go/issues/35722


If you have any questions or suggestions let me know.

Best,