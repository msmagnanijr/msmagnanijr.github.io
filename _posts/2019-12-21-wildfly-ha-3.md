---
layout: post
title: Building a Mission-Critical Open Source Environment for the Java Platform - Chapter III
tags: [jboss, wildfly, ha, redhat, centos, modcluster]
comments: true
---

In this chapter we will integrate WildFly and Mod Cluster. 

Below are the chapters I and II:

* [Building a Mission-Critical Open Source Environment for the Java Platform - Chapter I](http://mlab.run/2019/12/18/wildfly-ha-1)
* [Building a Mission-Critical Open Source Environment for the Java Platform - Chapter II](http://mlab.run/2019/12/21/wildfly-ha-2)

##  WildFly with Mod Cluster

If you followed the previous chapters, you will now have Apach Web Server HA in your environment as well as WildFly running in Domain Mode with four WildFly instances.

The first step will be to add two new properties to our WildFly instances. The properties must be added within each instance (server). So let's start by editing the host-slave.xml file in "Slave 0". 

{% highlight bash %}
[root@server-slave-0 ~]# pwd
/usr/local/wildfly/wildfly-18.0.1.Final
[root@server-slave-0 ~]# vi slave0/configuration/host-slave.xml 
{% endhighlight %}

{% highlight xml %}
##around line 89
    <servers>
        <server name="server-marketing-0" group="marketing">
            <system-properties>
                <property name="jboss.node.name" value="node-marketing-0" boot-time="true"/>
                <property name="wildfly.balancer.name" value="marketing-lb" boot-time="true"/>
            </system-properties>
        </server>
        <server name="server-accounting-0" group="accounting">
            <system-properties>
                <property name="jboss.node.name" value="node-accounting-0" boot-time="true"/>
                <property name="wildfly.balancer.name" value="accounting-lb" boot-time="true"/>
            </system-properties>
            <socket-bindings port-offset="100"/>
        </server>
    </servers>
##suppressed
{% endhighlight %}

Now perform the same procedure for "Slave1".

{% highlight bash %}
[root@server-slave-1 ~]# pwd
/usr/local/wildfly/wildfly-18.0.1.Final
[root@server-slave-1 ~]# vi slave1/configuration/host-slave.xml 
{% endhighlight %}

{% highlight xml %}
##around line 89
    <servers>
        <server name="server-marketing-1" group="marketing">
            <system-properties>
                <property name="jboss.node.name" value="node-marketing-1" boot-time="true"/>
                <property name="wildfly.balancer.name" value="marketing-lb" boot-time="true"/>
            </system-properties>
        </server>
        <server name="server-accounting-1" group="accounting">
            <system-properties>
                <property name="jboss.node.name" value="node-accounting-1" boot-time="true"/>
                <property name="wildfly.balancer.name" value="accounting-lb" boot-time="true"/>
            </system-properties>
            <socket-bindings port-offset="100"/>
        </server>
    </servers>
##suppressed
{% endhighlight %}

As you may have noticed, every configuration of the technologies available in WildFly is done  in the "Master". Remember that we currently have two groups "marketing" and "accounting" that are using the profile "full-ha" and socket-binding "full-ha-sockets".

Then in the "Master" edit the domain.xml file and add a new socket with our VIP defined in chapter I.

{% highlight bash %}
[root@server-domain ~]# pwd
/usr/local/wildfly/wildfly-18.0.1.Final
[root@server-domain ~]# vi master/configuration/domain.xml 
{% endhighlight %}

{% highlight xml %}
##around line 1880 // full-ha-sockets
        <outbound-socket-binding name="proxy">
            <remote-destination host="10.0.0.90" port="9090"/>
        </outbound-socket-binding>
##suppressed
{% endhighlight %}

Within the "full-ha" profile edit the modcluster subsystem and add the instance-id and balancer properties.

{% highlight xml %}
##around line 1658 // profile full-ha
        <proxy name="default" advertise-socket="modcluster" listener="ajp" proxies="proxy" balancer="${wildfly.balancer.name}">
##suppressed
{% endhighlight %}

Restart the WildFly service on the master and then on the slaves.

{% highlight bash %}
[root@server-domain ~]# systemctl restart wildfly
{% endhighlight %}

{% highlight bash %}
[root@server-slave-0 ~]# systemctl restart wildfly
{% endhighlight %}

{% highlight bash %}
[root@server-slave-1 ~]# systemctl restart wildfly
{% endhighlight %}

Access the VIP IP on the Mod Cluster port and context: http://10.0.0.90:9090/mod_cluster_manager. See that our instances are now connected and ready to use:

![](/images/201912-hawildfly-12.png)

Deploy the [cluster.war](https://github.com/msmagnanijr/mlab-run-blog-files/blob/master/wildfly-ha/cluster.war) application. Open the management console and go to "deployments" --> "content-repository" and upload the application. After that "deploy" --> "Deploy Content" --> choose "marketing" server group.

If the application is successfully deployed, you will see the context in "mod_cluster_manager": http://10.0.0.90:9090/mod_cluster_manager

![](/images/201912-hawildfly-13.png)


Best, #WIP